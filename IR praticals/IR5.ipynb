{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CBVj5q4yDVqT"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZFI8x9pNFtke"
   },
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = set()\n",
    "\n",
    "    for link in soup.find_all('a',href=True):\n",
    "      href = link.get('href')\n",
    "      if href.startswith('http'):\n",
    "        links.add(href)\n",
    "    return links\n",
    "  except requests.RequestException as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wowm9ryDGcRF"
   },
   "outputs": [],
   "source": [
    "def build_graph(start_urls,max_pages=10):\n",
    "  page_urls = []\n",
    "  to_crawl =  set(start_urls)\n",
    "  crawled = set()\n",
    "  url_to_links = {}\n",
    "  while to_crawl and len(crawled) < max_pages:\n",
    "    url = to_crawl.pop()\n",
    "    if url in crawled:\n",
    "      continue\n",
    "    print(f\"Crawling {url}\")\n",
    "    crawled.add(url)\n",
    "\n",
    "    links = get_links(url)\n",
    "    url_to_links[url] = links\n",
    "    for link in links:\n",
    "      to_crawl.add(link)\n",
    "    page_urls.append(url)\n",
    "    print(f\"Links found: {len(links)}, To crawl: {len(to_crawl)}, Crawled: {len(crawled)}\")\n",
    "  n = len(page_urls)\n",
    "  adjacency_matrix = np.zeros((n,n))\n",
    "  url_to_index = {url : idx for idx, url in enumerate(page_urls)}\n",
    "  for i, url in enumerate(page_urls):\n",
    "    links = url_to_links[url]\n",
    "    for link in links:\n",
    "      if link in url_to_index:\n",
    "        adjacency_matrix[i,url_to_index[link]] = 1\n",
    "  return page_urls,adjacency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "buKHocHOHTRE"
   },
   "outputs": [],
   "source": [
    "def pagerank(adjacency_matrix, d=0.85, max_iter=100, tol=1.0e-6):\n",
    "    N = len(adjacency_matrix)\n",
    "    column_sums = adjacency_matrix.sum(axis=0)\n",
    "    column_sums[column_sums == 0] = 1  # Avoid division by zero\n",
    "    M = adjacency_matrix / column_sums\n",
    "  # Normalize adjacency matrix (Column stochastic matrix)\n",
    "\n",
    "    # Handle dangling nodes (pages with no outgoing links)\n",
    "    dangling_nodes = np.where(M.sum(axis=0) == 0)[0]\n",
    "    M[:, dangling_nodes] = 1.0 / N  # Treat dangling nodes as if they are connected to all nodes\n",
    "\n",
    "    # Initial PageRank vector (uniform distribution)\n",
    "    rank = np.ones(N) / N\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        new_rank = (1 - d) / N + d * M.dot(rank)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(new_rank - rank, 1) < tol:\n",
    "            return new_rank\n",
    "\n",
    "        rank = new_rank\n",
    "\n",
    "    raise ValueError(f\"PageRank did not converge within {max_iter} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8QDRWI49IM8L"
   },
   "outputs": [],
   "source": [
    "start_urls = ['https://news.ycombinator.com/',\n",
    "'https://www.example.com/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZJ8qWuLIHn3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling https://news.ycombinator.com/\n"
     ]
    }
   ],
   "source": [
    "page_urls,adjacency_matrix = build_graph(start_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uK1jsj59IG4U"
   },
   "outputs": [],
   "source": [
    "page_rank_values = pagerank(adjacency_matrix,max_iter=1000,tol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbRYDzhKH3x-"
   },
   "outputs": [],
   "source": [
    "print(\"PageRank Scores:\")\n",
    "for url, rank in sorted(zip(page_urls, page_rank_values), key=lambda x: -x[1]):\n",
    "    print(f\"{url}: {rank}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
